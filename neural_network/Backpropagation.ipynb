{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.engine import data_adapter\n",
    "from tensorflow.python.profiler import trace\n",
    "from tensorflow.python.keras import callbacks as callbacks_module\n",
    "from tensorflow.python.keras.engine import training_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_BATCH_SIZE = 32\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, to_categorical(y_train))).batch(GLOBAL_BATCH_SIZE, ) #steps_per_epoch\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, to_categorical(y_test))).batch(GLOBAL_BATCH_SIZE, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape = (num_features,), kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1)),\n",
    "    tf.keras.layers.Dense(4, activation='relu', kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1) ),\n",
    "    tf.keras.layers.Dense(2, activation='sigmoid', kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1) ),\n",
    "]\n",
    "\n",
    "layers_clone = copy.deepcopy(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison: keras model vs. customized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reference: https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit#a_first_simple_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "for layer in layers_clone:\n",
    "    model.add(layer)\n",
    "\n",
    "Loss = BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=1e-2)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = Loss, metrics = ['acc'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = tf.keras.models.clone_model(model)\n",
    "model_custom.set_weights(model.get_weights())\n",
    "\n",
    "weights_custom = copy.deepcopy(model_custom.trainable_variables)\n",
    "\n",
    "Loss_custom = copy.deepcopy(Loss)\n",
    "optimizer_custom = copy.deepcopy(optimizer)\n",
    "\n",
    "model_custom.compile(optimizer = optimizer_custom, loss = [Loss_custom], metrics = ['acc'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_wei = []\n",
    "grads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00th batch loss: 0.95477647\n",
      "01th batch loss: 0.82564175\n",
      "02th batch loss: 0.802238\n",
      "03th batch loss: 0.85181713\n",
      "04th batch loss: 0.77963877\n",
      "05th batch loss: 0.7995611\n",
      "06th batch loss: 0.7566784\n",
      "07th batch loss: 0.72384125\n",
      "08th batch loss: 0.7095673\n",
      "09th batch loss: 0.7580291\n",
      "10th batch loss: 0.70888746\n",
      "11th batch loss: 0.67985237\n",
      "12th batch loss: 0.683792\n",
      "13th batch loss: 0.79725015\n",
      "14th batch loss: 0.670355\n"
     ]
    }
   ],
   "source": [
    "ith_batch = 0\n",
    "for xt, yt in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "#         tape.reset()\n",
    "        f = model_custom(xt, training=True)\n",
    "        loss = model_custom.compiled_loss(yt, f, regularization_losses=model_custom.losses)\n",
    "        print('{}th batch loss:'.format(str(ith_batch).zfill(2)), loss.numpy())\n",
    "#         loss = Loss(yt, f)\n",
    "\n",
    "    grad = tape.gradient(loss, model_custom.trainable_variables)\n",
    "    optimizer_custom.apply_gradients(zip(grad, model_custom.trainable_variables))\n",
    "    \n",
    "    ith_batch+=1\n",
    "    opt_wei.append(optimizer.weights)\n",
    "    grads.append(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step - loss: 0.7721 - acc: 0.3187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28bd0cb4550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, shuffle=False, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 2.9802322e-08\n",
      "True 2.3283064e-10\n",
      "True 7.450581e-09\n",
      "True 0.0\n",
      "True 9.313226e-10\n",
      "True 3.7252903e-09\n"
     ]
    }
   ],
   "source": [
    "for iw in range(len(model_custom.weights)):\n",
    "    print(\n",
    "#         np.alltrue(m.weights[iw] == model.weights[iw]), \n",
    "          np.allclose(model_custom.weights[iw], model.weights[iw]), abs(model_custom.weights[iw] - model.weights[iw]).numpy().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "(array([ True]), array([1], dtype=int64))\n",
      "1 1.4901161e-08\n",
      "(array([ True]), array([480], dtype=int64))\n",
      "2 7.450581e-09\n",
      "(array([ True]), array([16], dtype=int64))\n",
      "3 1.4901161e-08\n",
      "(array([ True]), array([64], dtype=int64))\n",
      "4 0.0\n",
      "(array([ True]), array([4], dtype=int64))\n",
      "5 1.4901161e-08\n",
      "(array([ True]), array([8], dtype=int64))\n",
      "6 0.0\n",
      "(array([ True]), array([2], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for iow in range(len(model_custom.optimizer.weights)):\n",
    "    print(iow, abs(model.optimizer.weights[iow] -  model_custom.optimizer.weights[iow]).numpy().max())\n",
    "#     print(np.unique((model.optimizer.weights[iow]== optimizer.weights[iow]), return_counts=True))\n",
    "    print(np.unique(np.isclose(model.optimizer.weights[iow], model_custom.optimizer.weights[iow], rtol=1e-5), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nth_layer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = copy.deepcopy(weights_custom[nth_layer])\n",
    "for it in range(len(grads)):\n",
    "    h = (tf.sqrt(opt_wei[it][nth_layer+1] )   + optimizer_custom.epsilon  ) \n",
    "    pred = pred - (optimizer_custom.learning_rate * (grads[it][nth_layer]/h))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = model.trainable_variables[nth_layer]\n",
    "# true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(tf.reshape(true - pred, [-1,]).numpy(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([436,  44], dtype=int64))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.isclose(true , pred), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reference: https://www.tensorflow.org/tutorials/distribute/custom_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = Sequential().distribute_strategy #tf.distribute.MirroredStrategy()\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 34954303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_wei = []\n",
    "# grads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "#         print(x.numpy().shape)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "#             loss = self.loss(y, y_pred,)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        print('..')\n",
    "        \n",
    "#         opt_wei.append(self.optimizer.weights)\n",
    "#         grads.append(gradients)\n",
    "        \n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layers, seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    inputs = tf.keras.Input(shape=(num_features,))\n",
    "    for layer_index, layer in enumerate(layers):\n",
    "        if layer_index == 0:\n",
    "            x = layer(inputs)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "#     x = tf.keras.layers.Dense(16, activation='relu', kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1))(inputs)\n",
    "#     x = tf.keras.layers.Dense(4, activation='relu', kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1) )(x)\n",
    "#     x = tf.keras.layers.Dense(2, activation='sigmoid', kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1) )(x)\n",
    "    model_ = CustomModel(inputs, x)\n",
    "    \n",
    "    return model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(seed):\n",
    "#     tf.random.set_seed(seed)\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     model.add(tf.keras.layers.Dense(16, activation='relu', input_shape = (num_features,), kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1)))\n",
    "#     model.add(tf.keras.layers.Dense(4, activation='relu', kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1) ))\n",
    "#     model.add(tf.keras.layers.Dense(2, activation='sigmoid', kernel_initializer = tf.keras.initializers.GlorotNormal(seed=1) ))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_clone = copy.deepcopy(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # Set reduction to `none` so we can do the reduction afterwards and divide by\n",
    "  # global batch size.\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "#       from_logits=True,\n",
    "#       reduction=tf.keras.losses.Reduction.NONE\n",
    "                  )\n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "#     test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "#     train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "#     test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model_custom1 = create_model(layer_clone, seed)\n",
    "    weights_custom1 = copy.deepcopy(model_custom1.trainable_variables)\n",
    "\n",
    "    optimizer_custom1 = tf.keras.optimizers.Adagrad(1e-2)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer_custom1, model=model_custom1)\n",
    "    model_custom1.compile(optimizer=optimizer_custom1, loss = loss_object, \n",
    "                          metrics = ['acc']\n",
    "                         )\n",
    "\n",
    "    model_custom1.train_function = model_custom1.make_train_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(inputs):\n",
    "#     images, labels = inputs\n",
    "\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model_custom1(images, training=True)\n",
    "#         loss = compute_loss(labels, predictions)\n",
    "\n",
    "#     gradients = tape.gradient(loss, model_custom1.trainable_variables)\n",
    "#     optimizer_custom1.apply_gradients(zip(gradients, model_custom1.trainable_variables))\n",
    "\n",
    "# #     train_accuracy.update_state(labels, predictions)\n",
    "#     return loss \n",
    "\n",
    "# @tf.function\n",
    "# def distributed_train_step(dataset_inputs):\n",
    "#   per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
    "#   return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "#                          axis=None)\n",
    "\n",
    "# @tf.function\n",
    "# def distributed_test_step(dataset_inputs):\n",
    "#   return strategy.run(test_step, args=(dataset_inputs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n",
      "..\n",
      "  Batch: 000, loss: 0.8209180235862732, acc: 0.375\n",
      "  Batch: 001, loss: 0.900952935218811, acc: 0.328125\n",
      "  Batch: 002, loss: 0.8808948993682861, acc: 0.3229166567325592\n",
      "  Batch: 003, loss: 0.8859827518463135, acc: 0.3125\n",
      "  Batch: 004, loss: 0.8820684552192688, acc: 0.33125001192092896\n",
      "  Batch: 005, loss: 0.872550904750824, acc: 0.3385416567325592\n",
      "  Batch: 006, loss: 0.8528721928596497, acc: 0.3526785671710968\n",
      "  Batch: 007, loss: 0.8475878834724426, acc: 0.35546875\n",
      "  Batch: 008, loss: 0.8376209139823914, acc: 0.3541666567325592\n",
      "  Batch: 009, loss: 0.8294330835342407, acc: 0.35624998807907104\n",
      "  Batch: 010, loss: 0.819271445274353, acc: 0.375\n",
      "  Batch: 011, loss: 0.8170177340507507, acc: 0.3854166567325592\n",
      "  Batch: 012, loss: 0.812931478023529, acc: 0.39423078298568726\n",
      "  Batch: 013, loss: 0.8097837567329407, acc: 0.3883928656578064\n",
      "  Batch: 014, loss: 0.8098143339157104, acc: 0.3890109956264496\n",
      "Epoch: 01, loss: 0.8453133702278137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # TRAIN LOOP\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    train_iter = iter(train_dist_dataset)\n",
    "    \n",
    "    for x in train_dataset:\n",
    "#       print(distributed_train_step(x), distributed_train_step(next(train_iter)))\n",
    "#       total_loss += model_custom1.train_function()\n",
    "\n",
    "        add_loss = model_custom1.train_function(train_iter) #distributed_train_step(next(train_iter))\n",
    "        print('  Batch: {}, loss: {}, acc: {}'.format(str(num_batches).zfill(3), add_loss['loss'].numpy(), add_loss['acc'].numpy()), )\n",
    "        \n",
    "        total_loss += add_loss['loss']\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss = total_loss / num_batches\n",
    "\n",
    "  # TEST LOOP\n",
    "#   for x in test_dist_dataset:\n",
    "#     distributed_test_step(x)\n",
    "\n",
    "#   if epoch % 2 == 0:\n",
    "#     checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "    print('Epoch: {}, loss: {}'.format(str(epoch+1).zfill(2), train_loss), )\n",
    "    \n",
    "#     template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "#               \"Test Accuracy: {}\")\n",
    "  \n",
    "#   print (template.format(epoch+1, train_loss,\n",
    "#                          train_accuracy.result()*100, test_loss.result(),\n",
    "#                          test_accuracy.result()*100))\n",
    "\n",
    "#   test_loss.reset_states()\n",
    "#   train_accuracy.reset_states()\n",
    "#   test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reference: https://github.com/tensorflow/tensorflow/blob/07acf0f4e07678a63667e0aad88ce2485b31d619/tensorflow/python/keras/engine/training.py#L1136-L1196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\n",
      " step: 0 loss, acc: 0.820918 0.375\n",
      " 1/15 [=>............................] - ETA: 2s - loss: 0.8209 - acc: 0.3750\n",
      " step: 1 loss, acc: 0.90095294 0.328125\n",
      "\n",
      " step: 2 loss, acc: 0.8808949 0.32291666\n",
      "\n",
      " step: 3 loss, acc: 0.88598275 0.3125\n",
      "\n",
      " step: 4 loss, acc: 0.88206846 0.33125\n",
      "\n",
      " step: 5 loss, acc: 0.8725509 0.33854166\n",
      "\n",
      " step: 6 loss, acc: 0.8528722 0.35267857\n",
      "\n",
      " step: 7 loss, acc: 0.8475879 0.35546875\n",
      "\n",
      " step: 8 loss, acc: 0.8376209 0.35416666\n",
      "\n",
      " step: 9 loss, acc: 0.8294331 0.35625\n",
      "\n",
      " step: 10 loss, acc: 0.81927145 0.375\n",
      "\n",
      " step: 11 loss, acc: 0.81701773 0.38541666\n",
      "\n",
      " step: 12 loss, acc: 0.8129315 0.39423078\n",
      "\n",
      " step: 13 loss, acc: 0.80978376 0.38839287\n",
      "\n",
      " step: 14 loss, acc: 0.80981433 0.389011\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(seed)\n",
    "layer_clone = copy.deepcopy(layers)\n",
    "\n",
    "model = Sequential()\n",
    "for layer in layer_clone:\n",
    "    model.add(layer)\n",
    "\n",
    "Loss = BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=1e-2)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = Loss, metrics = ['acc'], )\n",
    "\n",
    "\n",
    "batch_size=None\n",
    "verbose=1\n",
    "callbacks=None\n",
    "validation_split=0.\n",
    "validation_data=None\n",
    "shuffle=False\n",
    "class_weight=None\n",
    "sample_weight=None\n",
    "initial_epoch=0\n",
    "steps_per_epoch=None\n",
    "validation_steps=None\n",
    "validation_batch_size=None\n",
    "validation_freq=1\n",
    "max_queue_size=10\n",
    "workers=1\n",
    "use_multiprocessing=False\n",
    "\n",
    "with model.distribute_strategy.scope(), training_utils.RespectCompiledTrainableState(model):\n",
    "  # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\n",
    "    data_handler = data_adapter.get_data_handler(\n",
    "      x=train_dataset,\n",
    "#           y=y,\n",
    "      sample_weight=sample_weight,\n",
    "      batch_size=batch_size,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      initial_epoch=initial_epoch,\n",
    "      epochs=EPOCHS,\n",
    "      shuffle=shuffle,\n",
    "      class_weight=class_weight,\n",
    "      max_queue_size=max_queue_size,\n",
    "      workers=workers,\n",
    "      use_multiprocessing=use_multiprocessing,\n",
    "      model=model,\n",
    "      steps_per_execution=model._steps_per_execution)\n",
    "\n",
    "  # Container that configures and calls `tf.keras.Callback`s.\n",
    "    if not isinstance(callbacks, callbacks_module.CallbackList):\n",
    "        callbacks = callbacks_module.CallbackList(\n",
    "            callbacks,\n",
    "            add_history=True,\n",
    "            add_progbar=verbose != 0,\n",
    "            model=model,\n",
    "            verbose=verbose,\n",
    "            epochs=EPOCHS,\n",
    "            steps=data_handler.inferred_steps)\n",
    "\n",
    "    model.stop_training = False\n",
    "    model.train_function = model.make_train_function()\n",
    "    model._train_counter.assign(0)\n",
    "    callbacks.on_train_begin()\n",
    "    training_logs = None\n",
    "    # Handle fault-tolerance for multi-worker.\n",
    "    # TODO(omalleyt): Fix the ordering issues that mean this has to\n",
    "    # happen after `callbacks.on_train_begin`.\n",
    "    data_handler._initial_epoch = (  # pylint: disable=protected-access\n",
    "      model._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n",
    "    logs = None\n",
    "    for epoch, iterator in data_handler.enumerate_epochs():\n",
    "        print('epoch',epoch)\n",
    "    #     master, copy1 = itertools.tee(iterator)\n",
    "    #     ms = copy.copy(copy1)\n",
    "    #     print(len(list(ms)))\n",
    "    #     itr = copy.deepcopy(list(iterator))\n",
    "    #     print(len(list(iterator)))\n",
    "        model.reset_metrics()\n",
    "        callbacks.on_epoch_begin(epoch)\n",
    "        with data_handler.catch_stop_iteration():\n",
    "            for step in data_handler.steps():\n",
    "                with trace.Trace(\n",
    "                    'train',\n",
    "                    epoch_num=epoch,\n",
    "                    step_num=step,\n",
    "                    batch_size=batch_size,\n",
    "                    _r=1):\n",
    "#           master, copy1 = itertools.tee(iterator)\n",
    "#           ms = copy.copy(copy1)\n",
    "#           print(len(list(copy1)))\n",
    "                    callbacks.on_train_batch_begin(step)\n",
    "                    tmp_logs = model.train_function(iterator)\n",
    "                    print('\\n step:',step,'loss, acc:', tmp_logs['loss'].numpy(), tmp_logs['acc'].numpy())\n",
    "            \n",
    "                    if data_handler.should_sync: context.async_wait()\n",
    "                    logs = tmp_logs  # No error, now safe to assign to logs.\n",
    "                    end_step = step + data_handler.step_increment\n",
    "                    callbacks.on_train_batch_end(end_step, logs)\n",
    "                    if model.stop_training:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for iw in range(len(model.weights)):\n",
    "    print(np.alltrue(model.weights[iw] == model_custom1.weights[iw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ True]), array([1], dtype=int64))\n",
      "(array([ True]), array([480], dtype=int64))\n",
      "(array([ True]), array([16], dtype=int64))\n",
      "(array([ True]), array([64], dtype=int64))\n",
      "(array([ True]), array([4], dtype=int64))\n",
      "(array([ True]), array([8], dtype=int64))\n",
      "(array([ True]), array([2], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for iow in range(len(model.optimizer.weights)):\n",
    "    print(np.unique((model.optimizer.weights[iow] == model_custom1.optimizer.weights[iow]), return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nth_layer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = copy.deepcopy(weights_custom1[nth_layer])\n",
    "for it in range(len(grads)):\n",
    "    h = (tf.sqrt(opt_wei[it][nth_layer+1] )   + model_custom1.optimizer.epsilon  ) \n",
    "    pred = pred - (model_custom1.optimizer.learning_rate * (grads[it][nth_layer]/h))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = (tf.sqrt(optimizer.weights[nth_layer+1])  + optimizer.epsilon   ) \n",
    "# pred = w[nth_layer] - ((optimizer.learning_rate * grad[nth_layer])/h)    \n",
    "# # pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = model.trainable_variables[nth_layer]\n",
    "# true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
