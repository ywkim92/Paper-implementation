{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from treelib import Tree\n",
    "\n",
    "from sklearn.datasets import load_boston, load_breast_cancer\n",
    "from sklearn.tree import plot_tree, DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "feature_names = data['feature_names']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_tree` 함수로 트리를 그려보면 각 노드의 상태가 표기되어 있습니다. 세부 사항은 [포스트](https://ywkim92.github.io/machine_learning/feature_importance/#%EC%8B%9C%EA%B0%81%ED%99%94)에 기재해두었습니다. 다른 값들에 비해 유독 두 번째 줄 즉 feature와 threshold가 어째서 그렇게 결정되는지 이해하기 어려웠습니다. `random_state`가 바뀌어도 분기 기준은 바뀌지 않았으므로 무작위는 아니었습니다. \n",
    "\n",
    "먼저 분기 때마다 feature가 어떻게 정해지는지 찾아봤습니다. 검색 능력이 신통치 않아 적당한 레퍼런스를 발견하지 못했습니다. 다음엔 가설을 세우고 검증을 진행했습니다. 학습 데이터의 변수 중 분산이 가장 큰 (독립)변수? unique value 개수가 가장 많은 변수? (regression tree의 경우) 레이블과의 피어슨 상관계수가 가장 높은 변수? 모두 아니었습니다.\n",
    "\n",
    "그러다 Feature importance를 공부하던 중 무릎을 쳤습니다. 핵심은 **불순도 감소량(impurity decrease)** 이었습니다. scikit-learn의 의사결정나무 모델에서 parameter `splitter = 'best'`(default)로 설정했을 경우, 불순도를 가장 크게 감소시키는 feature와 threshold를 탐색해 분기 기준을 결정합니다.\n",
    "\n",
    "간단한 회귀 트리 모델을 생성하고 위 주장을 검증해보겠습니다. 앞서 분할한 train data를 학습합니다. 최상위 root node의 분기 기준은 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature for splitting root node: RM\n",
      "Threshold for splitting root node: 6.940999984741211\n"
     ]
    }
   ],
   "source": [
    "model_dt = DecisionTreeRegressor(max_leaf_nodes=10, random_state=42, )\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "print('Feature for splitting root node:', feature_names[model_dt.tree_.feature[0]])\n",
    "print('Threshold for splitting root node:', model_dt.tree_.threshold[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best splitter를 찾기 위해 brute force 방법을 사용했습니다.\n",
    "1. 독립변수 A를 선택한다.  \n",
    "2. 학습 데이터에서 해당 독립변수의 unique values를 추출하고 내림차순으로 정렬한다.  \n",
    "3. 모든 인접한 두 unique values 사이의 평균을 계산한다. 예를 들어, uniuqe values가 `[4, 3, 2, 1]`이라면, 계산 결과는 `[3.5, 2.5, 1.5]`이다.  \n",
    "4. 3번에서 얻은 리스트의 각 원소를 threshold value로 두고 split했을 때 불순도 감소량을 관찰한다. 불순도를 가장 크게 감소시키는 원소를 독립변수 A의 threshold로 정한다.(불순도 감소량 산출식은 [지난 포스트](https://ywkim92.github.io/machine_learning/feature_importance/#feature-importance) 참조)  \n",
    "5. 모든 독립변수에 대해 1~4번을 반복한다.  \n",
    "6. 불순도 감소량이 가장 큰 독립변수와 그 threshold로 해당 노드의 샘플을 split한다.\n",
    "\n",
    "위 방법에 따라 코딩하고 root node의 분기 기준을 도출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_splitter_reg(node_samples, feature_names, criterion):\n",
    "    '''Find the best splitter for a decision tree regressor.\n",
    "    \n",
    "    node_samples: train data(pandas DataFrame) whose the last column is label and its column name is 'label'\n",
    "    '''\n",
    "    n = node_samples.shape[0] # the number of samples for training\n",
    "    dic= dict()\n",
    "    for col in feature_names:\n",
    "        tr = node_samples[[col, 'label']]\n",
    "        nt = tr.shape[0]\n",
    "        it = tr['label'].var()\n",
    "        values = np.unique(tr[col])\n",
    "        if values.size==1:\n",
    "            continue\n",
    "        thresholds = np.array([values[i:i+2].mean() for i in range(values.size-1)])[::-1]\n",
    "\n",
    "        if values.size <=10:\n",
    "            iter_lim = thresholds.size\n",
    "        else:\n",
    "            iter_lim = thresholds.size//2\n",
    "\n",
    "        i_max = 0\n",
    "        th_tag = 0\n",
    "        lim = 0\n",
    "        for th in thresholds:\n",
    "            # right node\n",
    "            trr = tr[(tr[col]>th)]\n",
    "            ntr = trr.shape[0]\n",
    "            # calculate impurity for regression tree; 'mae' or 'mse'(default)\n",
    "            if criterion == 'mae':\n",
    "                ir = np.abs(trr['label'] - np.median(trr['label'])).mean()\n",
    "            else:\n",
    "                ir = trr['label'].var()\n",
    "\n",
    "            # left node\n",
    "            trl = tr[tr[col]<=th]\n",
    "            ntl = trl.shape[0]\n",
    "            if criterion == 'mae':\n",
    "                il = np.abs(trl['label'] - np.median(trl['label'])).mean()\n",
    "            else:\n",
    "                il = trl['label'].var()\n",
    "\n",
    "            # calculate weighted impurity decrease\n",
    "            i = (nt / n) * ( it - (ntl / nt) * il - (ntr / nt)* ir )\n",
    "\n",
    "            if i > i_max:\n",
    "                i_max = i\n",
    "                th_tag = th\n",
    "\n",
    "            lim+=1\n",
    "            if lim==iter_lim:\n",
    "                break\n",
    "\n",
    "        dic[col] = (i_max, th_tag)\n",
    "    \n",
    "    best_feature_ = max(dic, key = lambda x: dic[x][0])\n",
    "    best_threshold_ = dic[best_feature_][1]\n",
    "    return best_feature_, best_threshold_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 경우의 수에 대해 불순도 감소량을 산출하다 보니 scikit-learn 모델에 비해 시간이 너무 오래 걸렸습니다. unique value의 중앙값 부근에서는 불순도가 정체되는 양상이 확인되므로 unique values의 상위 50%에 대해서만 계산하게끔 처리했습니다. `iter_lim`을 정의하고 그 값을 넘으면 for문을 종료했습니다. best splitter를 찾는 알고리즘에 대해서는 추가 리서치할 계획입니다.\n",
    "\n",
    "* 정의한 `best_splitter_reg` 함수로 도출한 분기 기준은 scikit-learn의 결과와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best splitter of the root node(feature, threshold): ('RM', 6.941)\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame(np.hstack((X_train, y_train.reshape(-1,1))), columns=feature_names.tolist()+['label'])\n",
    "\n",
    "print('The best splitter of the root node(feature, threshold):',best_splitter_reg(train, feature_names, model_dt.criterion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* root node에서 분기된 두 노드에 대해 검증하여도 scikit-learn과 같은 결과를 얻습니다.  \n",
    "1. for node #1(splitted to left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using scikit-learn attributes\n",
      "Feature for splitting the 1st node: LSTAT\n",
      "Threshold for splitting the 1st node: 14.400000095367432\n",
      "\n",
      "* Using implementation\n",
      "The best splitter of the 1st node(feature, threshold): ('LSTAT', 14.399999999999999)\n"
     ]
    }
   ],
   "source": [
    "print('* Using scikit-learn attributes')\n",
    "print('Feature for splitting the 1st node:', feature_names[model_dt.tree_.feature[1]])\n",
    "print('Threshold for splitting the 1st node:', model_dt.tree_.threshold[1],)\n",
    "\n",
    "print('\\n* Using implementation')\n",
    "train_node1 = train[train['RM']<=6.941]\n",
    "print('The best splitter of the 1st node(feature, threshold):', best_splitter_reg(train_node1, feature_names, model_dt.criterion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. for node #2(splitted to right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using scikit-learn attributes\n",
      "Feature for splitting the 2nd node: RM\n",
      "Threshold for splitting the 2nd node: 7.437000036239624\n",
      "\n",
      "* Using implementation\n",
      "The best splitter of the 1st node(feature, threshold): ('RM', 7.436999999999999)\n"
     ]
    }
   ],
   "source": [
    "print('* Using scikit-learn attributes')\n",
    "print('Feature for splitting the 2nd node:', feature_names[model_dt.tree_.feature[2]])\n",
    "print('Threshold for splitting the 2nd node:', model_dt.tree_.threshold[2],)\n",
    "\n",
    "print('\\n* Using implementation')\n",
    "train_node2 = train[train['RM']>6.941]\n",
    "print('The best splitter of the 1st node(feature, threshold):', best_splitter_reg(train_node2, feature_names, model_dt.criterion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `criterion = 'mae'`으로 설정했을 때도 마찬가지입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when criterion = \"mae\"\n",
      "* Using scikit-learn attributes\n",
      "Feature for splitting root node: RM\n",
      "Threshold for splitting root node: 6.797000169754028\n",
      "\n",
      "* Using implementation\n",
      "The best splitter of the root node(feature, threshold): ('RM', 6.797)\n"
     ]
    }
   ],
   "source": [
    "model_dt1 = DecisionTreeRegressor(max_leaf_nodes=10, random_state=42, criterion='mae')\n",
    "model_dt1.fit(X_train, y_train)\n",
    "\n",
    "print('when criterion = \"mae\"')\n",
    "print('* Using scikit-learn attributes')\n",
    "print('Feature for splitting root node:', feature_names[model_dt1.tree_.feature[0]])\n",
    "print('Threshold for splitting root node:', model_dt1.tree_.threshold[0])\n",
    "\n",
    "print('\\n* Using implementation')\n",
    "print('The best splitter of the root node(feature, threshold):',best_splitter_reg(train, feature_names, model_dt1.criterion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to assign Value for each node: 노드에 속하는 샘플에 부여되는 label 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 회귀: 해당 노드에 속하는 샘플들의 평균입니다. 각 노드에 부여되는 label 값을 산출하는 코드를 짜서 검증해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_value_reg(X_train, y_train, tree_regressor):\n",
    "    result = []\n",
    "    n_node = tree_regressor.tree_.node_count\n",
    "    for i in range(n_node):\n",
    "        value = y_train[tree_regressor.decision_path(X_train).toarray()[:, i]==1].mean()\n",
    "        result.append(value)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklean == my implementation: True\n"
     ]
    }
   ],
   "source": [
    "tree_values_sklearn = model_dt.tree_.value.flatten()\n",
    "tree_values_imp = tree_value_reg(X_train, y_train, model_dt)\n",
    "print('Sklean == my implementation:',np.allclose(tree_values_sklearn, tree_values_imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분류: 각 class를 index로 하며 해당 노드에 속하는 class별 샘플의 개수를 원소로 하는 list입니다. 이는 predict 단계에서 class probabilities를 계산할 때 사용됩니다. 예를 들어 이진 분류 모델에서 leaf node N의 value가 `[2, 8]`이라면, leaf node N에 최종 도달한 샘플이 class 0일 확률은 0.2, class 1일 확률은 0.8로 계산됩니다. threshold가 0.5라면 이 샘플은 class 1로 분류되겠지요.  \n",
    "코드로 구현한 후 그 결과를 scikit-learn 결과와 비교해봅니다. 검증 결과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_value_clf(X_train, y_train, tree_clf):\n",
    "    n_class = tree_clf.n_classes_\n",
    "    result = np.array([]).reshape(-1, n_class)\n",
    "    n_node = tree_clf.tree_.node_count\n",
    "    \n",
    "    for i in range(n_node):\n",
    "        uniq, value = np.unique(y_train[tree_clf.decision_path(X_train).toarray()[:, i]==1], return_counts=True)\n",
    "        if value.size == n_class:\n",
    "            result = np.vstack((result, value))\n",
    "        else:\n",
    "            value_ = np.zeros(n_class)\n",
    "            value_[np.array(uniq, dtype=int)] = value\n",
    "            result = np.vstack((result, value_))\n",
    "        \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증에 사용할 이진 분류 데이터 샘플(유방암 데이터)과 의사결정나무 분류 모델을 불러옵니다. 검증 결과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklean == my implementation: True\n"
     ]
    }
   ],
   "source": [
    "data_clf = load_breast_cancer()\n",
    "X_clf = data_clf['data']\n",
    "y_clf = data_clf['target']\n",
    "\n",
    "model_clf = DecisionTreeClassifier(max_leaf_nodes=10, random_state=42)\n",
    "model_clf.fit(X_clf, y_clf)\n",
    "\n",
    "tree_values_sklearn = model_clf.tree_.value[:, 0, :]\n",
    "tree_values_imp = tree_value_clf(X_clf, y_clf, model_clf)\n",
    "print('Sklean == my implementation:',np.allclose(tree_values_sklearn, tree_values_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
